{
  "name": "Louis.GitHub.webcollector",
  "tagline": "WebCollector 爬虫框架",
  "body": "https://github.com/CrawlScript/WebCollector\r\n\r\n# WebCollector\r\n\r\n爬虫简介\r\nWebCollector是一个无须配置、便于二次开发的JAVA爬虫框架（内核），它提供精简的的API，只需少量代码即可实现一个功能强大的爬虫。\r\n爬虫内核：\r\nWebCollector致力于维护一个稳定、可扩的爬虫内核，便于开发者进行灵活的二次开发。内核具有很强的扩展性，用户可以在内核基础上开发自己想要的爬虫。源码中集成了Jsoup，可进行精准的网页解析。\r\nDEMO：\r\n用WebCollector制作一个爬取《知乎》并进行问题精准抽取的爬虫（JAVA）\r\nimport java.io.IOException;\r\nimport java.util.regex.Pattern;\r\n\r\nimport org.jsoup.nodes.Document;\r\n\r\nimport cn.edu.hfut.dmic.webcollector.model.CrawlDatums;\r\nimport cn.edu.hfut.dmic.webcollector.model.Page;\r\nimport cn.edu.hfut.dmic.webcollector.plugin.berkeley.BreadthCrawler;\r\n\r\npublic class zhihuCrawler extends BreadthCrawler {\r\n\r\n\tpublic zhihuCrawler(String crawlPath, boolean autoParse) {\r\n\t\tsuper(crawlPath, autoParse);\r\n\t\t// TODO Auto-generated constructor stub\r\n\t}\r\n\r\n\t/* 启动爬虫 */\r\n\tpublic static void main(String[] args) throws IOException {\r\n\t\tzhihuCrawler crawler = new zhihuCrawler(\"crsw\", true);\r\n\t\ttry {\r\n\t\t\tcrawler.addSeed(\"http://www.zhihu.com/question/21003086\");\r\n\t\t\tcrawler.addRegex(\"http://www.zhihu.com/question/.*\");\r\n\t\t\tcrawler.start(5);\r\n\t\t} catch (Exception e) {\r\n\t\t\t// TODO: handle exception\r\n\t\t}\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void visit(Page page, CrawlDatums arg1) {\r\n\t\t// TODO Auto-generated method stub\r\n\t\tString question_regex = \"^http://www.zhihu.com/question/[0-9]+\";\r\n\t\tif (Pattern.matches(question_regex, page.getUrl())) {\r\n\t\t\tSystem.out.println(\"正在抽取\" + page.getUrl());\r\n\t\t\t/* 抽取标题 */\r\n\t\t\tString title = page.doc().title();\r\n\t\t\tSystem.out.println(title);\r\n\t\t\t/* 抽取提问内容 */\r\n\t\t\tString question = page.doc().select(\"div[id=zh-question-detail]\").text();\r\n\t\t\tSystem.out.println(question);\r\n\r\n\t\t}\r\n\t}\r\n}\r\n",
  "google": "",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}